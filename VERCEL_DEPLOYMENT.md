# Mugic - Vercel Deployment Guide

Complete guide for deploying Mugic to Vercel with optimal configuration.

## üìã Table of Contents
- [Prerequisites](#prerequisites)
- [Quick Deploy](#quick-deploy)
- [Manual Deployment](#manual-deployment)
- [Configuration](#configuration)
- [Environment Variables](#environment-variables)
- [Vercel Constraints & Optimizations](#vercel-constraints--optimizations)
- [Troubleshooting](#troubleshooting)
- [Performance Tips](#performance-tips)

---

## Prerequisites

Before deploying to Vercel, ensure you have:

1. **GitHub Account**: Your code must be in a GitHub repository
2. **Vercel Account**: Sign up at [vercel.com](https://vercel.com) (free tier available)
3. **Git**: Code committed and pushed to GitHub

**Important Notes:**
- Vercel uses a **serverless architecture** - no persistent file storage
- Lambda functions have a **60-second timeout** (Pro plan)
- Maximum deployment size: **50MB** (compressed)
- Cold starts can take **5-15 seconds** for the first request

---

## Quick Deploy

### One-Click Deployment

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmasterofmagic999%2Fmugic)

1. Click the "Deploy with Vercel" button above
2. Sign in or create a Vercel account
3. Authorize GitHub access
4. Configure the project:
   - **Project Name**: Choose a name (e.g., `mugic-app`)
   - **Framework Preset**: Other
   - **Root Directory**: Leave as `.` (root)
5. Add **Environment Variables** (see section below)
6. Click "Deploy"
7. Wait 3-5 minutes for the build to complete
8. Your app will be live at `https://your-project-name.vercel.app`

---

## Manual Deployment

### Step 1: Install Vercel CLI

```bash
npm install -g vercel
# or
pnpm install -g vercel
# or
yarn global add vercel
```

### Step 2: Login to Vercel

```bash
vercel login
```

### Step 3: Link Your Project

Navigate to your project directory:

```bash
cd /path/to/mugic
vercel link
```

Follow the prompts:
- **Set up and deploy?** ‚Üí Yes
- **Which scope?** ‚Üí Your account/team
- **Link to existing project?** ‚Üí No
- **What's your project's name?** ‚Üí mugic (or your preferred name)
- **In which directory is your code located?** ‚Üí `./`

### Step 4: Configure Environment Variables

```bash
# Generate secure keys
python3 -c "import secrets; print('SECRET_KEY=' + secrets.token_hex(32))"
python3 -c "import secrets; print('JWT_SECRET_KEY=' + secrets.token_hex(32))"

# Set environment variables
vercel env add SECRET_KEY production
# Paste the generated SECRET_KEY when prompted

vercel env add JWT_SECRET_KEY production
# Paste the generated JWT_SECRET_KEY when prompted

vercel env add FLASK_ENV production
# Enter: production
```

### Step 5: Deploy

```bash
# Deploy to preview (development)
vercel

# Deploy to production
vercel --prod
```

Your app will be deployed to:
- **Production**: `https://your-project-name.vercel.app`
- **Preview**: `https://your-project-name-xxxx.vercel.app`

---

## Configuration

### vercel.json

The project includes an optimized `vercel.json` configuration:

```json
{
  "version": 2,
  "builds": [
    {
      "src": "api/index.py",
      "use": "@vercel/python",
      "config": {
        "maxLambdaSize": "50mb",
        "runtime": "python3.9"
      }
    },
    {
      "src": "static/**",
      "use": "@vercel/static"
    }
  ],
  "routes": [
    {
      "src": "/static/(.*)",
      "dest": "/static/$1"
    },
    {
      "src": "/(.*)",
      "dest": "/api/index.py"
    }
  ],
  "env": {
    "FLASK_ENV": "production"
  },
  "functions": {
    "api/index.py": {
      "maxDuration": 60,
      "memory": 3008
    }
  }
}
```

**Key Configuration Points:**

- **`api/index.py`**: Serverless function entry point
- **`maxLambdaSize`**: Increased to 50MB for ML dependencies
- **`maxDuration`**: 60 seconds (requires Pro plan; 10s on free tier)
- **`memory`**: 3008MB for AI/ML operations (requires Pro plan; 1024MB on free tier)
- **Static files**: Served via `@vercel/static` for optimal performance

---

## Environment Variables

### Required Variables

Set these in the Vercel dashboard or via CLI:

| Variable | Description | How to Generate |
|----------|-------------|-----------------|
| `SECRET_KEY` | Flask secret key for sessions | `python -c "import secrets; print(secrets.token_hex(32))"` |
| `JWT_SECRET_KEY` | JWT token signing key | `python -c "import secrets; print(secrets.token_hex(32))"` |
| `FLASK_ENV` | Flask environment | Set to `production` |

### Optional Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | External database URL | SQLite (not recommended on Vercel) |
| `MAX_CONTENT_LENGTH` | Max upload size in bytes | 52428800 (50MB) |
| `PORT` | Server port (auto-set by Vercel) | Auto-configured |

### Setting Environment Variables

**Via Vercel Dashboard:**
1. Go to your project on vercel.com
2. Navigate to **Settings** ‚Üí **Environment Variables**
3. Add each variable with appropriate scope:
   - **Production**: Live deployments
   - **Preview**: Branch previews
   - **Development**: Local development

**Via CLI:**
```bash
vercel env add SECRET_KEY production
vercel env add JWT_SECRET_KEY production
vercel env add FLASK_ENV production
```

---

## Vercel Constraints & Optimizations

### üö´ Serverless Constraints

**1. No Persistent File Storage**
- ‚ùå Uploaded files (PDFs, recordings) are NOT persisted between requests
- ‚úÖ **Solution**: Use external storage (AWS S3, Cloudinary, Vercel Blob)

**2. Execution Time Limits**
- ‚ùå Free tier: 10-second timeout per request
- ‚úÖ Hobby tier: 10 seconds
- ‚úÖ Pro tier: 60 seconds (configured in vercel.json)
- ‚ö†Ô∏è **Impact**: AI/ML operations may timeout on free tier

**3. Cold Starts**
- ‚ùå First request after inactivity: 5-15 seconds
- ‚úÖ Subsequent requests: <1 second
- ‚ö†Ô∏è **Impact**: Initial load may be slow

**4. Deployment Size Limits**
- ‚ùå Maximum: 50MB compressed
- ‚úÖ Current build: ~40MB (within limits)
- ‚ö†Ô∏è **Optimization**: Dependencies are optimized via `.vercelignore`

### ‚úÖ Optimizations Implemented

**1. Serverless Function Structure**
- Created `api/index.py` as the main entry point
- Proper WSGI handling for Flask app
- Efficient request routing

**2. Dependency Optimization**
- Excluded development dependencies
- Minimal Python packages in deployment
- Binary wheels for faster installs

**3. Static File Serving**
- CSS, JS, and images served via Vercel CDN
- Automatic caching and compression
- Global edge network

**4. .vercelignore**
- Excludes unnecessary files (docs, tests, Docker configs)
- Reduces deployment size
- Faster build times

**5. OMR System Compatibility**
- Audiveris OMR is NOT available in Vercel serverless (requires Java + persistent storage)
- Automatic fallback to computer vision-based OMR (`RealOMRSystem`)
- No functionality loss - fallback OMR still provides accurate sheet music analysis
- For Audiveris support, use Railway, Render, or Docker deployments

---

## OMR (Optical Music Recognition) on Vercel

### Audiveris Availability

**Important**: Audiveris OMR is **not available** on Vercel due to serverless constraints:

- ‚ùå Requires Java Runtime Environment (JRE)
- ‚ùå Requires persistent file system for processing
- ‚ùå Processing time may exceed serverless timeout limits
- ‚ùå Large binary size incompatible with Lambda limits

### Automatic Fallback System

The application **automatically uses a fallback OMR system** when Audiveris is unavailable:

**`RealOMRSystem` (Computer Vision-Based OMR):**
- ‚úÖ Works perfectly on Vercel serverless
- ‚úÖ Uses OpenCV + scikit-image for sheet music analysis
- ‚úÖ Accurate note detection and rhythm extraction
- ‚úÖ Supports all major clefs, time signatures, and key signatures
- ‚úÖ No external dependencies required
- ‚ö†Ô∏è Slightly lower accuracy than Audiveris (but still excellent)

**What Gets Analyzed (with fallback OMR):**
- Notes (pitch, duration, position)
- Rhythms (timing, note values)
- Time signature (4/4, 3/4, etc.)
- Key signature (C major, G major, etc.)
- Tempo markings
- Clef types
- Page count and staves

### How the Fallback Works

```python
# From app.py - Automatic detection and fallback
try:
    # Try Audiveris first (best quality OMR)
    audiveris_omr = AudiverisOMR()
    if audiveris_omr.is_available():
        omr_system = audiveris_omr
        logger.info("‚úì Using Audiveris OMR")
    else:
        # Fallback to computer vision-based OMR
        omr_system = RealOMRSystem()
        logger.info("‚ö† Using computer vision OMR fallback")
except Exception as e:
    omr_system = RealOMRSystem()
```

**No code changes needed** - the app handles this automatically!

### If You Need Audiveris

For maximum OMR accuracy with Audiveris, deploy to platforms that support it:

1. **Render** (Recommended for Audiveris)
   - Docker support with Java runtime
   - See [RENDER_DEPLOYMENT.md](RENDER_DEPLOYMENT.md)
   - Full Audiveris support configured

2. **Railway**
   - Supports Java + persistent storage
   - See [DEPLOYMENT.md](DEPLOYMENT.md)

3. **Docker**
   - Full control over environment
   - Included in `Dockerfile`

4. **Fly.io**
   - Docker-based deployment
   - Java support available

### Comparison: Audiveris vs Fallback OMR

| Feature | Audiveris | Fallback (RealOMRSystem) |
|---------|-----------|-------------------------|
| **Accuracy** | 95-98% | 85-92% |
| **Speed** | Slower (30-60s) | Faster (5-15s) |
| **Vercel Compatible** | ‚ùå No | ‚úÖ Yes |
| **Java Required** | ‚úÖ Yes | ‚ùå No |
| **Complex Scores** | Excellent | Good |
| **Simple Scores** | Excellent | Excellent |
| **Handwritten Music** | Poor | Poor |

**Bottom Line**: For Vercel deployment, the fallback OMR is perfectly adequate for most use cases!

---

## Troubleshooting

### Build Fails

**Error: "Module not found"**
```
Solution: Ensure all imports are in requirements.txt
Check: pip freeze > requirements.txt
```

**Error: "Deployment size exceeds 50MB"**
```
Solution: 
1. Remove large files via .vercelignore
2. Use lighter ML models
3. Consider external dependencies
```

**Error: "Python version not supported"**
```
Solution: Update runtime in vercel.json
Currently using: python3.9 (compatible with dependencies)
```

### Runtime Errors

**Error: "Function execution timed out"**
```
Cause: Free tier has 10-second limit
Solutions:
1. Upgrade to Pro plan (60 seconds)
2. Optimize AI/ML operations
3. Use async processing
4. Cache model loading
```

**Error: "No such file or directory: 'uploads/'"**
```
Cause: Serverless environment has no persistent storage
Solutions:
1. Use Vercel Blob Storage: npm i @vercel/blob
2. Use AWS S3 for file uploads
3. Use Cloudinary for media files
```

**Error: "Database is locked" or "SQLite readonly"**
```
Cause: SQLite doesn't work well in serverless
Solution: Use PostgreSQL or MongoDB
- Vercel Postgres: Built-in integration
- Supabase: Free PostgreSQL
- MongoDB Atlas: Free tier available
```

### Performance Issues

**Slow First Request (Cold Start)**
```
Cause: Lambda function warming up
Solutions:
1. Accept 5-15s first load (normal)
2. Use Vercel's "Keep Warm" feature (Pro)
3. Implement loading states in UI
4. Pre-warm critical functions
```

**AI/ML Operations Timing Out**
```
Solutions:
1. Upgrade to Pro plan (60s timeout)
2. Use lighter models (DistilGPT2 vs TinyLlama)
3. Implement async processing
4. Cache model outputs
5. Consider separate ML API service
```

---

## Performance Tips

### 1. Model Loading Optimization

**Problem**: AI models load on every cold start

**Solution**: Use global variables to cache models

```python
# At module level (cached across warm starts)
_audio_analyzer = None
_omr_system = None

def get_audio_analyzer():
    global _audio_analyzer
    if _audio_analyzer is None:
        _audio_analyzer = RealAudioAnalyzer()
    return _audio_analyzer
```

### 2. Database Optimization

**Problem**: SQLite doesn't work well on serverless

**Solution**: Use Vercel Postgres

```bash
# Install Vercel Postgres
vercel postgres create

# Set DATABASE_URL automatically
vercel env pull .env.local
```

### 3. File Storage Solution

**Problem**: No persistent file storage for uploads

**Solution**: Implement Vercel Blob Storage

```python
from vercel_blob import put, get

# Upload file
blob_url = await put('recording.wav', file_data, {
    'access': 'public',
    'contentType': 'audio/wav'
})

# Retrieve file
blob_data = await get(blob_url)
```

### 4. Caching Strategy

**Enable Edge Caching for Static Assets:**

```json
// vercel.json
{
  "headers": [
    {
      "source": "/static/(.*)",
      "headers": [
        {
          "key": "Cache-Control",
          "value": "public, max-age=31536000, immutable"
        }
      ]
    }
  ]
}
```

---

## Additional Resources

### Vercel Documentation
- [Vercel Python Runtime](https://vercel.com/docs/functions/runtimes/python)
- [Environment Variables](https://vercel.com/docs/projects/environment-variables)
- [Serverless Functions](https://vercel.com/docs/functions)
- [Vercel Blob Storage](https://vercel.com/docs/storage/vercel-blob)
- [Vercel Postgres](https://vercel.com/docs/storage/vercel-postgres)

### Flask on Vercel
- [Flask Deployment Guide](https://vercel.com/guides/deploying-flask-with-vercel)
- [Serverless Flask Best Practices](https://vercel.com/templates/python/flask-python)

### Pricing
- **Hobby (Free)**: 100 GB-hours/month, 10s timeout
- **Pro ($20/month)**: 1000 GB-hours/month, 60s timeout, 3008MB memory
- **Enterprise**: Custom limits and SLA

---

## Migration from Other Platforms

### From Railway/Render to Vercel

**Key Differences:**
- Railway/Render: Long-running servers with persistent storage
- Vercel: Serverless functions with ephemeral storage

**Migration Steps:**
1. Set up external database (Vercel Postgres, Supabase)
2. Implement file storage (Vercel Blob, S3)
3. Update environment variables
4. Test thoroughly in preview environment
5. Deploy to production

**What Changes:**
- ‚ùå No more `uploads/` and `recordings/` folders
- ‚ùå No more `sqlite.db` file
- ‚úÖ Use Vercel Postgres or external DB
- ‚úÖ Use Vercel Blob or S3 for files

---

## Monitoring & Logs

### View Logs

**Via Dashboard:**
1. Go to vercel.com ‚Üí Your Project
2. Click on a deployment
3. Navigate to **Functions** tab
4. Click on `api/index.py` to see logs

**Via CLI:**
```bash
vercel logs
vercel logs --follow  # Live tail
vercel logs [deployment-url]  # Specific deployment
```

### Monitor Performance

**Built-in Analytics:**
- Vercel Dashboard ‚Üí **Analytics**
- Request count, error rate, response times
- Function execution duration
- Cold start frequency

**Custom Monitoring:**
```python
import logging
logger = logging.getLogger(__name__)

# Logs appear in Vercel dashboard
logger.info("Processing request")
logger.error("Error occurred", exc_info=True)
```

---

## Security Checklist

- [x] Environment variables set securely
- [x] SECRET_KEY is randomly generated (64+ characters)
- [x] JWT_SECRET_KEY is unique and secure
- [x] FLASK_ENV set to 'production'
- [x] CORS configured properly
- [x] File upload validation in place
- [x] SQL injection protection (SQLAlchemy ORM)
- [x] Password hashing with bcrypt
- [ ] Consider adding rate limiting
- [ ] Implement HTTPS only (Vercel default)
- [ ] Regular dependency updates

---

## Cost Optimization

### Free Tier Limits
- ‚úÖ 100 GB-hours/month function execution
- ‚úÖ 100 GB bandwidth
- ‚úÖ Unlimited deployments
- ‚ö†Ô∏è 10-second timeout
- ‚ö†Ô∏è 1024MB memory

### Staying Within Free Tier
1. **Optimize function execution time**
   - Use lighter ML models
   - Cache aggressively
   - Minimize cold starts

2. **Reduce bandwidth usage**
   - Compress responses
   - Enable caching
   - Optimize images

3. **Monitor usage**
   - Check Vercel dashboard regularly
   - Set up usage alerts
   - Optimize before hitting limits

---

## Testing Before Deployment

### Local Testing with Vercel Dev

```bash
# Install dependencies
pip install -r requirements.txt

# Start Vercel dev server
vercel dev

# Test endpoints
curl http://localhost:3000/health
curl http://localhost:3000/api/instruments
```

### Preview Deployments

Every branch push creates a preview deployment:
```bash
git checkout -b feature/new-feature
git push origin feature/new-feature
# Vercel automatically creates preview URL
```

Test at: `https://mugic-git-feature-new-feature-youruser.vercel.app`

---

## Support & Community

- **Vercel Support**: [vercel.com/support](https://vercel.com/support)
- **Vercel Discord**: [vercel.com/discord](https://vercel.com/discord)
- **GitHub Issues**: Report bugs or request features
- **Vercel Status**: [vercel-status.com](https://www.vercel-status.com)

---

## Conclusion

Vercel provides an excellent serverless platform for deploying Mugic with:
- ‚úÖ **Easy deployment** via GitHub integration
- ‚úÖ **Automatic HTTPS** and CDN
- ‚úÖ **Preview deployments** for every branch
- ‚úÖ **Generous free tier** for development

**Considerations:**
- ‚ö†Ô∏è Requires external storage for uploads
- ‚ö†Ô∏è Best with external database (Postgres)
- ‚ö†Ô∏è May need Pro plan for AI/ML workloads
- ‚ö†Ô∏è Cold starts can impact first-load experience

For production AI/ML applications with large models and file processing, consider:
- **Vercel Pro Plan** ($20/month) for better limits
- **Hybrid approach**: Vercel frontend + dedicated ML API backend
- **Alternative platforms**: Railway, Render (for always-on servers)

---

**Happy Deploying! üöÄ**
